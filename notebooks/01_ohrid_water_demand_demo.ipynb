{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Demand Prediction for Ohrid, North Macedonia\n",
    "\n",
    "## Research Framework Demo\n",
    "\n",
    "This notebook demonstrates the comprehensive water demand prediction framework specifically designed for Ohrid, incorporating:\n",
    "\n",
    "- **Regional Characteristics**: Tourism patterns, Mediterranean climate, cultural factors\n",
    "- **Traditional Time Series**: ARIMA, SARIMA, Exponential Smoothing\n",
    "- **Machine Learning**: Random Forest, XGBoost, LightGBM\n",
    "- **Deep Learning**: Neural Networks, LSTM\n",
    "- **Hybrid Approaches**: Ensemble methods\n",
    "- **Comprehensive Evaluation**: Multiple metrics including peak demand analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_collectors.ohrid_synthetic_generator import OhridWaterDemandGenerator\n",
    "from models.ohrid_predictor import OhridWaterDemandPredictor\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(\"üìç Research focus: Ohrid, North Macedonia\")\n",
    "print(\"üéØ Objective: Water demand prediction using multiple approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Realistic Synthetic Data for Ohrid\n",
    "\n",
    "We'll generate synthetic water demand data that captures Ohrid's unique characteristics:\n",
    "\n",
    "- **Tourism Seasonality**: UNESCO World Heritage site with peak summer tourism\n",
    "- **Cultural Events**: Ohrid Summer Festival, Orthodox holidays\n",
    "- **Climate**: Mediterranean with continental influence\n",
    "- **Infrastructure**: Lake Ohrid as water source, mixed-age distribution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Ohrid-specific data generator\n",
    "generator = OhridWaterDemandGenerator(config_path='../config/ohrid_config.yaml')\n",
    "\n",
    "# Generate 3 years of hourly data (2021-2023)\n",
    "print(\"üîÑ Generating synthetic data for Ohrid (2021-2023)...\")\n",
    "synthetic_data = generator.generate_synthetic_data(\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2023-12-31\",\n",
    "    frequency=\"1h\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated {len(synthetic_data):,} hourly observations\")\n",
    "print(f\"üìä Data shape: {synthetic_data.shape}\")\n",
    "print(f\"üìÖ Date range: {synthetic_data['timestamp'].min()} to {synthetic_data['timestamp'].max()}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nüìà Water Demand Statistics:\")\n",
    "print(f\"Average demand: {synthetic_data['water_demand_m3_per_hour'].mean():.2f} m¬≥/hour\")\n",
    "print(f\"Peak demand: {synthetic_data['water_demand_m3_per_hour'].max():.2f} m¬≥/hour\")\n",
    "print(f\"Minimum demand: {synthetic_data['water_demand_m3_per_hour'].min():.2f} m¬≥/hour\")\n",
    "print(f\"Standard deviation: {synthetic_data['water_demand_m3_per_hour'].std():.2f} m¬≥/hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the generated data\n",
    "print(\"üîç Sample of generated data:\")\n",
    "display(synthetic_data.head(10))\n",
    "\n",
    "print(\"\\nüìã Data columns:\")\n",
    "for i, col in enumerate(synthetic_data.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Let's explore the patterns in our synthetic data to understand Ohrid's water demand characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot of water demand\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Water Demand Over Time',\n",
    "        'Seasonal Patterns',\n",
    "        'Daily Patterns',\n",
    "        'Tourism Impact'\n",
    "    ],\n",
    "    specs=[[{\"colspan\": 2}, None],\n",
    "           [{}, {}]]\n",
    ")\n",
    "\n",
    "# Overall time series (sample every 24 hours for clarity)\n",
    "sample_data = synthetic_data[::24].copy()  # Daily samples\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sample_data['timestamp'],\n",
    "        y=sample_data['water_demand_m3_per_hour'],\n",
    "        mode='lines',\n",
    "        name='Daily Demand',\n",
    "        line=dict(color='blue', width=1)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Monthly averages\n",
    "monthly_avg = synthetic_data.groupby('month')['water_demand_m3_per_hour'].mean()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=monthly_avg.index,\n",
    "        y=monthly_avg.values,\n",
    "        name='Monthly Average',\n",
    "        marker_color='lightblue'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Hourly patterns\n",
    "hourly_avg = synthetic_data.groupby('hour')['water_demand_m3_per_hour'].mean()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hourly_avg.index,\n",
    "        y=hourly_avg.values,\n",
    "        mode='lines+markers',\n",
    "        name='Hourly Pattern',\n",
    "        line=dict(color='red')\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Ohrid Water Demand Analysis\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Month\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Hour of Day\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Demand (m¬≥/hour)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Demand (m¬≥/hour)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Demand (m¬≥/hour)\", row=2, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tourism impact analysis\n",
    "tourism_analysis = synthetic_data.groupby(['month', 'is_tourist_season']).agg({\n",
    "    'water_demand_m3_per_hour': 'mean',\n",
    "    'tourists_estimated': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Water demand by tourist season\n",
    "sns.boxplot(data=synthetic_data, x='month', y='water_demand_m3_per_hour', \n",
    "            hue='is_tourist_season', ax=ax1)\n",
    "ax1.set_title('Water Demand by Month and Tourist Season')\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Water Demand (m¬≥/hour)')\n",
    "ax1.legend(title='Tourist Season')\n",
    "\n",
    "# Tourist numbers throughout the year\n",
    "monthly_tourists = synthetic_data.groupby('month')['tourists_estimated'].mean()\n",
    "ax2.plot(monthly_tourists.index, monthly_tourists.values, marker='o', linewidth=2, markersize=8)\n",
    "ax2.set_title('Average Tourist Numbers by Month')\n",
    "ax2.set_xlabel('Month')\n",
    "ax2.set_ylabel('Estimated Tourists')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üèñÔ∏è Tourism Impact Insights:\")\n",
    "print(f\"Peak tourist month: {monthly_tourists.idxmax()} (avg: {monthly_tourists.max():.0f} tourists)\")\n",
    "print(f\"Off-season month: {monthly_tourists.idxmin()} (avg: {monthly_tourists.min():.0f} tourists)\")\n",
    "tourist_demand_increase = (\n",
    "    synthetic_data[synthetic_data['is_tourist_season']]['water_demand_m3_per_hour'].mean() /\n",
    "    synthetic_data[~synthetic_data['is_tourist_season']]['water_demand_m3_per_hour'].mean() - 1\n",
    ") * 100\n",
    "print(f\"Demand increase during tourist season: +{tourist_demand_increase:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Feature Engineering\n",
    "\n",
    "Now we'll prepare the data for modeling using our comprehensive feature engineering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the predictor\n",
    "predictor = OhridWaterDemandPredictor(config_path='../config/ohrid_config.yaml')\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"üîß Applying feature engineering...\")\n",
    "df = predictor.load_data('../data/raw/ohrid_synthetic_water_demand.csv') if os.path.exists('../data/raw/ohrid_synthetic_water_demand.csv') else synthetic_data.copy()\n",
    "\n",
    "# Prepare data for modeling\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_cols = predictor.prepare_data_for_modeling(df)\n",
    "\n",
    "print(f\"üìä Data split summary:\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Validation samples: {len(X_val):,}\")\n",
    "print(f\"   Test samples: {len(X_test):,}\")\n",
    "print(f\"   Features: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\nüéØ Target variable statistics:\")\n",
    "print(f\"   Training mean: {y_train.mean():.2f} m¬≥/hour\")\n",
    "print(f\"   Training std: {y_train.std():.2f} m¬≥/hour\")\n",
    "print(f\"   Training range: {y_train.min():.2f} - {y_train.max():.2f} m¬≥/hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display engineered features\n",
    "print(\"üõ†Ô∏è Engineered Features:\")\n",
    "for i, feature in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = X_train.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\n‚ö†Ô∏è Missing values detected:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values in training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "We'll train multiple types of models:\n",
    "1. Traditional time series models (ARIMA, ETS)\n",
    "2. Machine learning models (Random Forest, XGBoost, LightGBM)\n",
    "3. Deep learning models (Neural Networks, LSTM)\n",
    "4. Hybrid ensemble approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Traditional Time Series Models\n",
    "print(\"üìà Training traditional time series models...\")\n",
    "print(\"   üîÑ ARIMA/SARIMA models...\")\n",
    "arima_models = predictor.fit_arima_models(y_train, seasonal=True)\n",
    "\n",
    "print(\"   üîÑ Exponential Smoothing...\")\n",
    "ets_models = predictor.fit_exponential_smoothing(y_train)\n",
    "\n",
    "print(f\"   ‚úÖ Fitted {len(arima_models) + len(ets_models)} time series models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Machine Learning Models\n",
    "print(\"ü§ñ Training machine learning models...\")\n",
    "print(\"   üîÑ Random Forest, XGBoost, LightGBM...\")\n",
    "ml_models = predictor.fit_machine_learning_models(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(f\"   ‚úÖ Fitted {len(ml_models)} machine learning models\")\n",
    "\n",
    "# Display feature importance for one model\n",
    "if 'RandomForest' in predictor.feature_importance:\n",
    "    print(\"\\nüîç Top 10 Most Important Features (Random Forest):\")\n",
    "    rf_importance = predictor.feature_importance['RandomForest']\n",
    "    top_features = sorted(rf_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for i, (feature, importance) in enumerate(top_features, 1):\n",
    "        print(f\"   {i:2d}. {feature:<30} {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Deep Learning Models\n",
    "print(\"üß† Training deep learning models...\")\n",
    "print(\"   üîÑ Neural Networks and LSTM...\")\n",
    "dl_models = predictor.fit_deep_learning_models(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(f\"   ‚úÖ Fitted {len(dl_models)} deep learning models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Hybrid Ensemble Model\n",
    "print(\"üîÄ Creating hybrid ensemble model...\")\n",
    "ensemble_models = predictor.create_hybrid_ensemble(X_train, y_train, ['RandomForest', 'XGBoost', 'LightGBM'])\n",
    "\n",
    "print(f\"   ‚úÖ Created ensemble with {len(predictor.models['Ensemble']['base_models'])} base models\")\n",
    "\n",
    "print(f\"\\nüéØ Total models trained: {len(predictor.models)}\")\n",
    "print(\"   Models:\", list(predictor.models.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Comparison\n",
    "\n",
    "We'll evaluate all models using multiple metrics relevant to water demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "print(\"üìä Evaluating models on test set...\")\n",
    "results = predictor.evaluate_models(X_test, y_test)\n",
    "\n",
    "print(f\"‚úÖ Evaluated {len(results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model comparison\n",
    "comparison_df = predictor.compare_models()\n",
    "\n",
    "# Create a more detailed visualization\n",
    "if not comparison_df.empty:\n",
    "    # Performance metrics visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=['Mean Absolute Error (MAE)', 'Root Mean Square Error (RMSE)', \n",
    "                       'Mean Absolute Percentage Error (MAPE)', 'R¬≤ Score'],\n",
    "        specs=[[{}, {}], [{}, {}]]\n",
    "    )\n",
    "    \n",
    "    metrics = ['MAE', 'RMSE', 'MAPE', 'R2']\n",
    "    positions = [(1,1), (1,2), (2,1), (2,2)]\n",
    "    \n",
    "    for metric, (row, col) in zip(metrics, positions):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=comparison_df.index,\n",
    "                y=comparison_df[metric],\n",
    "                name=metric,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=600, title_text=\"Model Performance Comparison\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Best models summary\n",
    "    print(\"\\nüèÜ BEST PERFORMING MODELS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_mae = comparison_df['MAE'].idxmin()\n",
    "    best_rmse = comparison_df['RMSE'].idxmin()\n",
    "    best_mape = comparison_df['MAPE'].idxmin()\n",
    "    best_r2 = comparison_df['R2'].idxmax()\n",
    "    best_peak = comparison_df['Peak_MAE'].idxmin()\n",
    "    best_direction = comparison_df['Directional_Accuracy'].idxmax()\n",
    "    \n",
    "    print(f\"üéØ Overall Accuracy (MAE): {best_mae} ({comparison_df.loc[best_mae, 'MAE']:.3f})\")\n",
    "    print(f\"üìê Precision (RMSE): {best_rmse} ({comparison_df.loc[best_rmse, 'RMSE']:.3f})\")\n",
    "    print(f\"üìä Percentage Error (MAPE): {best_mape} ({comparison_df.loc[best_mape, 'MAPE']:.2f}%)\")\n",
    "    print(f\"üîó Explained Variance (R¬≤): {best_r2} ({comparison_df.loc[best_r2, 'R2']:.3f})\")\n",
    "    print(f\"‚ö° Peak Demand Accuracy: {best_peak} ({comparison_df.loc[best_peak, 'Peak_MAE']:.3f})\")\n",
    "    print(f\"üìà Directional Accuracy: {best_direction} ({comparison_df.loc[best_direction, 'Directional_Accuracy']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis\n",
    "\n",
    "Understanding which features drive water demand predictions in Ohrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance visualization\n",
    "predictor.plot_feature_importance(top_n=15)\n",
    "\n",
    "# Create a combined feature importance analysis\n",
    "if len(predictor.feature_importance) > 0:\n",
    "    print(\"\\nüîç FEATURE IMPORTANCE INSIGHTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Combine importance scores from all models\n",
    "    all_features = set()\n",
    "    for model_importance in predictor.feature_importance.values():\n",
    "        all_features.update(model_importance.keys())\n",
    "    \n",
    "    # Calculate average importance across models\n",
    "    avg_importance = {}\n",
    "    for feature in all_features:\n",
    "        importances = []\n",
    "        for model_importance in predictor.feature_importance.values():\n",
    "            if feature in model_importance:\n",
    "                importances.append(model_importance[feature])\n",
    "        if importances:\n",
    "            avg_importance[feature] = np.mean(importances)\n",
    "    \n",
    "    # Top features\n",
    "    top_avg_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"Top 10 Most Important Features (Average across models):\")\n",
    "    for i, (feature, importance) in enumerate(top_avg_features, 1):\n",
    "        print(f\"   {i:2d}. {feature:<35} {importance:.4f}\")\n",
    "    \n",
    "    # Categorize features\n",
    "    feature_categories = {\n",
    "        'Temporal': ['hour', 'day_of_week', 'month', 'is_weekend', 'is_holiday', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos', 'month_sin', 'month_cos'],\n",
    "        'Weather': ['temperature', 'humidity', 'precipitation', 'wind_speed', 'pressure', 'cloud_cover'],\n",
    "        'Tourism': ['tourists_estimated', 'is_tourist_season', 'is_festival_period'],\n",
    "        'Historical': [f for f in all_features if 'lag_' in f or 'rolling_' in f],\n",
    "        'Interaction': [f for f in all_features if 'interaction' in f]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä Feature Category Importance:\")\n",
    "    for category, features in feature_categories.items():\n",
    "        category_importance = sum(avg_importance.get(f, 0) for f in features if f in avg_importance)\n",
    "        print(f\"   {category:<15}: {category_importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Visualization\n",
    "\n",
    "Visual analysis of model predictions vs actual demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions visualization\n",
    "print(\"üìà Generating prediction visualizations...\")\n",
    "\n",
    "# Select best performing models for visualization\n",
    "if not comparison_df.empty:\n",
    "    # Get top 4 models by MAE\n",
    "    top_models = comparison_df.nsmallest(4, 'MAE').index.tolist()\n",
    "    predictor.plot_predictions(y_test, model_names=top_models, days_to_show=14)\n",
    "    \n",
    "    print(f\"üìä Showing predictions for top 4 models: {', '.join(top_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for best model\n",
    "if not comparison_df.empty:\n",
    "    best_model = comparison_df['MAE'].idxmin()\n",
    "    best_predictions = predictor.evaluation_results[best_model]['predictions']\n",
    "    residuals = y_test.values - best_predictions\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Residual Analysis - {best_model} (Best Model)', fontsize=16)\n",
    "    \n",
    "    # Residuals over time\n",
    "    axes[0,0].plot(residuals[:168])  # First week\n",
    "    axes[0,0].set_title('Residuals Over Time (First Week)')\n",
    "    axes[0,0].set_xlabel('Hours')\n",
    "    axes[0,0].set_ylabel('Residual (m¬≥/hour)')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual distribution\n",
    "    axes[0,1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0,1].set_title('Residual Distribution')\n",
    "    axes[0,1].set_xlabel('Residual (m¬≥/hour)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    axes[1,0].scatter(y_test.values, best_predictions, alpha=0.5, s=1)\n",
    "    axes[1,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[1,0].set_title('Predicted vs Actual')\n",
    "    axes[1,0].set_xlabel('Actual (m¬≥/hour)')\n",
    "    axes[1,0].set_ylabel('Predicted (m¬≥/hour)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1,1])\n",
    "    axes[1,1].set_title('Q-Q Plot (Normality Check)')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Residual Analysis for {best_model}:\")\n",
    "    print(f\"   Mean residual: {residuals.mean():.4f} m¬≥/hour\")\n",
    "    print(f\"   Std residual: {residuals.std():.4f} m¬≥/hour\")\n",
    "    print(f\"   Max absolute residual: {abs(residuals).max():.4f} m¬≥/hour\")\n",
    "    \n",
    "    # Shapiro-Wilk test for normality (on a sample)\n",
    "    sample_residuals = np.random.choice(residuals, min(5000, len(residuals)), replace=False)\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(sample_residuals)\n",
    "    print(f\"   Normality test (Shapiro-Wilk): p-value = {shapiro_p:.4f}\")\n",
    "    print(f\"   Residuals {'appear normal' if shapiro_p > 0.05 else 'deviate from normality'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Peak Demand Analysis\n",
    "\n",
    "Special focus on peak demand periods which are critical for infrastructure planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak demand analysis\n",
    "print(\"‚ö° Peak Demand Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define peak periods (top 10% of demand)\n",
    "peak_threshold = y_test.quantile(0.9)\n",
    "peak_mask = y_test >= peak_threshold\n",
    "peak_periods = y_test[peak_mask]\n",
    "\n",
    "print(f\"Peak demand threshold: {peak_threshold:.2f} m¬≥/hour\")\n",
    "print(f\"Number of peak demand hours: {peak_mask.sum()} ({peak_mask.mean()*100:.1f}% of test period)\")\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    # Peak demand performance comparison\n",
    "    peak_performance = {}\n",
    "    \n",
    "    for model_name in predictor.evaluation_results.keys():\n",
    "        predictions = predictor.evaluation_results[model_name]['predictions']\n",
    "        peak_predictions = predictions[peak_mask]\n",
    "        \n",
    "        if len(peak_predictions) > 0:\n",
    "            peak_mae = mean_absolute_error(peak_periods, peak_predictions)\n",
    "            peak_mape = np.mean(np.abs((peak_periods - peak_predictions) / peak_periods)) * 100\n",
    "            \n",
    "            peak_performance[model_name] = {\n",
    "                'Peak_MAE': peak_mae,\n",
    "                'Peak_MAPE': peak_mape\n",
    "            }\n",
    "    \n",
    "    peak_df = pd.DataFrame(peak_performance).T\n",
    "    peak_df = peak_df.sort_values('Peak_MAE')\n",
    "    \n",
    "    print(\"\\nüèÜ Peak Demand Performance Ranking:\")\n",
    "    print(peak_df.round(3))\n",
    "    \n",
    "    best_peak_model = peak_df.index[0]\n",
    "    print(f\"\\n‚≠ê Best model for peak demand: {best_peak_model}\")\n",
    "    print(f\"   Peak MAE: {peak_df.loc[best_peak_model, 'Peak_MAE']:.3f} m¬≥/hour\")\n",
    "    print(f\"   Peak MAPE: {peak_df.loc[best_peak_model, 'Peak_MAPE']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tourism Impact Assessment\n",
    "\n",
    "Analysis of how well models capture tourism-driven demand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tourism impact on model performance\n",
    "print(\"üèñÔ∏è Tourism Impact Assessment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get test data with tourism information\n",
    "test_data = df.iloc[-len(X_test):].copy()\n",
    "test_data['actual_demand'] = y_test.values\n",
    "\n",
    "# Add predictions from best model\n",
    "if not comparison_df.empty:\n",
    "    best_model = comparison_df['MAE'].idxmin()\n",
    "    test_data['predicted_demand'] = predictor.evaluation_results[best_model]['predictions']\n",
    "    test_data['prediction_error'] = abs(test_data['actual_demand'] - test_data['predicted_demand'])\n",
    "    \n",
    "    # Performance during different tourism periods\n",
    "    tourism_performance = test_data.groupby('is_tourist_season').agg({\n",
    "        'prediction_error': ['mean', 'std'],\n",
    "        'actual_demand': 'mean',\n",
    "        'tourists_estimated': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"Model Performance by Tourism Season:\")\n",
    "    print(tourism_performance)\n",
    "    \n",
    "    # Festival periods performance\n",
    "    festival_performance = test_data.groupby('is_festival_period').agg({\n",
    "        'prediction_error': ['mean', 'std'],\n",
    "        'actual_demand': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"\\nModel Performance during Festival Periods:\")\n",
    "    print(festival_performance)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Tourism season comparison\n",
    "    tourism_data = []\n",
    "    for season in [True, False]:\n",
    "        season_data = test_data[test_data['is_tourist_season'] == season]\n",
    "        tourism_data.append(season_data['prediction_error'])\n",
    "    \n",
    "    axes[0].boxplot(tourism_data, labels=['Tourist Season', 'Off Season'])\n",
    "    axes[0].set_title('Prediction Error by Tourism Season')\n",
    "    axes[0].set_ylabel('Absolute Error (m¬≥/hour)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Monthly performance\n",
    "    monthly_error = test_data.groupby('month')['prediction_error'].mean()\n",
    "    axes[1].bar(monthly_error.index, monthly_error.values)\n",
    "    axes[1].set_title('Monthly Prediction Error')\n",
    "    axes[1].set_xlabel('Month')\n",
    "    axes[1].set_ylabel('Mean Absolute Error (m¬≥/hour)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight tourist season months\n",
    "    tourist_months = [6, 7, 8, 5, 9]  # Peak and shoulder seasons\n",
    "    for month in tourist_months:\n",
    "        if month in monthly_error.index:\n",
    "            axes[1].patches[month-1].set_color('orange')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Key insights\n",
    "    tourist_error = test_data[test_data['is_tourist_season']]['prediction_error'].mean()\n",
    "    off_season_error = test_data[~test_data['is_tourist_season']]['prediction_error'].mean()\n",
    "    \n",
    "    print(f\"\\nüìä Key Tourism Impact Insights:\")\n",
    "    print(f\"   Average error during tourist season: {tourist_error:.3f} m¬≥/hour\")\n",
    "    print(f\"   Average error during off season: {off_season_error:.3f} m¬≥/hour\")\n",
    "    \n",
    "    if tourist_error > off_season_error:\n",
    "        print(f\"   ‚ö†Ô∏è Model struggles more during tourist season (+{((tourist_error/off_season_error-1)*100):.1f}% higher error)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Model performs better during tourist season (-{((1-tourist_error/off_season_error)*100):.1f}% lower error)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Deployment Recommendations\n",
    "\n",
    "Summary and recommendations for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "print(\"üíæ Saving trained models...\")\n",
    "predictor.save_models(\"../models/\")\n",
    "\n",
    "# Generate deployment recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ MODEL DEPLOYMENT RECOMMENDATIONS FOR OHRID WATER UTILITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    # Overall best model\n",
    "    best_overall = comparison_df['MAE'].idxmin()\n",
    "    best_peak = comparison_df['Peak_MAE'].idxmin()\n",
    "    best_direction = comparison_df['Directional_Accuracy'].idxmax()\n",
    "    \n",
    "    print(f\"\\nüèÜ RECOMMENDED PRIMARY MODEL: {best_overall}\")\n",
    "    print(f\"   ‚Ä¢ Best overall accuracy (MAE: {comparison_df.loc[best_overall, 'MAE']:.3f} m¬≥/hour)\")\n",
    "    print(f\"   ‚Ä¢ MAPE: {comparison_df.loc[best_overall, 'MAPE']:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ R¬≤: {comparison_df.loc[best_overall, 'R2']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚ö° RECOMMENDED FOR PEAK DEMAND: {best_peak}\")\n",
    "    print(f\"   ‚Ä¢ Best peak demand accuracy (Peak MAE: {comparison_df.loc[best_peak, 'Peak_MAE']:.3f} m¬≥/hour)\")\n",
    "    \n",
    "    print(f\"\\nüìà RECOMMENDED FOR TREND ANALYSIS: {best_direction}\")\n",
    "    print(f\"   ‚Ä¢ Best directional accuracy ({comparison_df.loc[best_direction, 'Directional_Accuracy']:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüéØ DEPLOYMENT STRATEGY:\")\n",
    "    print(\"   1. Primary forecasting: Use\", best_overall, \"for daily operational planning\")\n",
    "    print(\"   2. Peak demand alerts: Use\", best_peak, \"for infrastructure stress monitoring\")\n",
    "    print(\"   3. Ensemble approach: Combine top 3 models for critical decisions\")\n",
    "    \n",
    "    print(\"\\nüìä MONITORING RECOMMENDATIONS:\")\n",
    "    print(\"   ‚Ä¢ Update models monthly with new data\")\n",
    "    print(\"   ‚Ä¢ Monitor tourism data for seasonal adjustments\")\n",
    "    print(\"   ‚Ä¢ Track prediction errors during festival periods\")\n",
    "    print(\"   ‚Ä¢ Implement automated alerts for prediction deviations >20%\")\n",
    "    \n",
    "    print(\"\\nüîß INFRASTRUCTURE CONSIDERATIONS:\")\n",
    "    print(\"   ‚Ä¢ Peak demand preparation during July-August (tourist season)\")\n",
    "    print(\"   ‚Ä¢ Network maintenance scheduling during low-demand periods (Nov-Mar)\")\n",
    "    print(\"   ‚Ä¢ Emergency capacity planning for festival periods (Ohrid Summer Festival)\")\n",
    "    \n",
    "    # Feature-based recommendations\n",
    "    if predictor.feature_importance:\n",
    "        print(\"\\nüìà DATA COLLECTION PRIORITIES:\")\n",
    "        all_importance = {}\n",
    "        for model_importance in predictor.feature_importance.values():\n",
    "            for feature, importance in model_importance.items():\n",
    "                all_importance[feature] = all_importance.get(feature, 0) + importance\n",
    "        \n",
    "        top_features = sorted(all_importance.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"   High priority data sources:\")\n",
    "        for i, (feature, _) in enumerate(top_features, 1):\n",
    "            if 'tourist' in feature.lower():\n",
    "                print(f\"   {i}. Tourism data (booking platforms, hotel occupancy)\")\n",
    "            elif 'temp' in feature.lower():\n",
    "                print(f\"   {i}. Weather stations (temperature monitoring)\")\n",
    "            elif 'lag' in feature.lower():\n",
    "                print(f\"   {i}. Historical demand data (automated meter readings)\")\n",
    "            elif 'hour' in feature.lower() or 'day' in feature.lower():\n",
    "                print(f\"   {i}. Temporal patterns (calendar integration)\")\n",
    "            else:\n",
    "                print(f\"   {i}. {feature} data\")\n",
    "\n",
    "print(\"\\n‚úÖ RESEARCH FRAMEWORK DEMONSTRATION COMPLETE!\")\n",
    "print(\"\\nüìö Next Steps for Real Implementation:\")\n",
    "print(\"   1. Collect real water demand data from Ohrid utility\")\n",
    "print(\"   2. Integrate live weather API feeds\")\n",
    "print(\"   3. Connect to tourism data sources\")\n",
    "print(\"   4. Deploy models to GCP for automated forecasting\")\n",
    "print(\"   5. Build dashboard for utility operators\")\n",
    "print(\"   6. Implement continuous model retraining pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated a comprehensive water demand prediction framework specifically tailored for Ohrid, North Macedonia. The framework successfully:\n",
    "\n",
    "### Key Achievements:\n",
    "1. **Generated realistic synthetic data** capturing Ohrid's unique characteristics\n",
    "2. **Implemented multiple modeling approaches** from traditional time series to deep learning\n",
    "3. **Evaluated models comprehensively** with metrics relevant to water utilities\n",
    "4. **Analyzed tourism impact** on demand patterns and model performance\n",
    "5. **Provided actionable recommendations** for deployment\n",
    "\n",
    "### Research Contributions:\n",
    "- **Regional Adaptation**: Framework specifically designed for Balkan/Mediterranean tourism-dependent cities\n",
    "- **Multi-modal Approach**: Combines traditional and modern forecasting techniques\n",
    "- **Tourism Integration**: Explicit modeling of UNESCO World Heritage site tourism patterns\n",
    "- **Peak Demand Focus**: Special attention to infrastructure-critical peak periods\n",
    "- **Practical Deployment**: Cloud-ready framework with GCP integration\n",
    "\n",
    "### Next Steps:\n",
    "The framework is ready for:\n",
    "1. **Real Data Integration**: Replace synthetic data with actual utility data\n",
    "2. **Production Deployment**: Deploy to GCP for operational use\n",
    "3. **Continuous Learning**: Implement automated model retraining\n",
    "4. **Dashboard Development**: Create operational interfaces for utility staff\n",
    "5. **Research Publication**: Document methodology and results for academic contribution\n",
    "\n",
    "This research provides a solid foundation for water demand prediction in tourism-dependent cities and can be adapted for similar locations worldwide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}